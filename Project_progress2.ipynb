{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674885e4-fcb3-40b5-984b-88499ce4b73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:19.674627Z",
     "start_time": "2022-02-27T04:40:07.664041Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b1b6b9-f3a5-4d3c-97cc-f5047faf0a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:19.903646Z",
     "start_time": "2022-02-27T04:40:19.681250Z"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1644645329784,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "5f7GoYC8K1M2"
   },
   "outputs": [],
   "source": [
    "item = pd.read_csv('item_feature.csv')\n",
    "train = pd.read_csv('training.csv')\n",
    "df = train.merge(item, on = 'item_id', how = 'left')\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2a8d04-7076-4689-9c82-5f51db315c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:27.979981Z",
     "start_time": "2022-02-27T04:40:19.914140Z"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1644645333677,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "hg1eeAaMLNu1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_ids = train['item_id'].unique()\n",
    "\n",
    "users, items, labels, cntxt_ftres = [], [], [], []\n",
    "\n",
    "user_item_set = set(zip(train['user_id'], train['item_id'], train['context_feature_id']))\n",
    "\n",
    "track = set(zip(train['user_id'], train['item_id']))\n",
    "\n",
    "\n",
    "for (u, i, c) in user_item_set:\n",
    "    users.append(u)\n",
    "    items.append(i)\n",
    "    cntxt_ftres.append(c)\n",
    "    labels.append(1)\n",
    "    negative_item = np.random.choice(item_ids) \n",
    "    while (u, negative_item) in track:\n",
    "        negative_item = np.random.choice(item_ids)\n",
    "    users.append(u)\n",
    "    items.append(negative_item)\n",
    "    cntxt_ftres.append(c)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeaf9c42-208c-4a7a-9bde-a0b6368fa7aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:31.565437Z",
     "start_time": "2022-02-27T04:40:29.837063Z"
    },
    "executionInfo": {
     "elapsed": 5291,
     "status": "ok",
     "timestamp": 1644645339795,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "_LrBIlsILT_b"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.Series(users),pd.Series(items),\n",
    "               pd.Series(cntxt_ftres), pd.Series(labels)], axis =1).\\\n",
    "rename(columns={0:'user_id', 1:'item_id', 2:'context_feature_id', 3:'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c67b5c6-8ad8-4afd-804d-05c737d4aa66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:33.397917Z",
     "start_time": "2022-02-27T04:40:33.275649Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.merge(item, on = 'item_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4443ac7-36c0-406b-ac5a-096084ec669d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:35.210361Z",
     "start_time": "2022-02-27T04:40:35.208339Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_sample(data):\n",
    "\n",
    "    train=data.sample(frac=0.8)\n",
    "    val=data.drop(train.index)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf4d9fb-784e-4bd9-9f24-1becfbbbfc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:37.311991Z",
     "start_time": "2022-02-27T04:40:37.052478Z"
    }
   },
   "outputs": [],
   "source": [
    "train , val = data_sample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1251cf",
   "metadata": {},
   "source": [
    "### Old Model: More parameters fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48b3b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:40.986860Z",
     "start_time": "2022-02-27T04:40:40.981675Z"
    }
   },
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=20):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "        self.nonlin = nn.LeakyReLU()\n",
    "        self.drop = nn.Dropout(p = 0.1)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        U = self.nonlin(U)\n",
    "        V = self.item_emb(v)\n",
    "        V = self.drop(V)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        self.U, self.V = U, V\n",
    "        return self.classifier((U*V).sum(1) +  b_u  + b_v)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.U[idx], self.V[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d918d6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:47:02.700209Z",
     "start_time": "2022-02-27T04:47:02.677908Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_df, optimizer):\n",
    "    \"\"\" Trains the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    ### BEGIN SOLUTION\n",
    "    y = torch.FloatTensor(train_df.label.values)\n",
    "    u = torch.LongTensor(train_df.user_id.values)\n",
    "    v = torch.LongTensor(train_df.item_id.values)\n",
    "    y_hat = model(u,v)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    train_acc = accuracy_score(output,y)\n",
    "    train_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    ### END SOLUTION\n",
    "    return train_loss.item(), train_acc\n",
    "\n",
    "def valid_metrics(model, valid_df):\n",
    "    \"\"\"Computes validation loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    ### BEGIN SOLUTION\n",
    "    u = torch.LongTensor(valid_df.user_id.values)\n",
    "    v = torch.LongTensor(valid_df.item_id.values)\n",
    "    y = torch.FloatTensor(valid_df.label.values)\n",
    "    y_hat = model(u,v)\n",
    "    valid_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    auc = roc_auc_score( y.detach().numpy(), y_hat.detach().numpy())\n",
    "    valid_acc = accuracy_score(output,y)\n",
    "    ### END SOLUTION\n",
    "    return valid_loss.item(), valid_acc, auc\n",
    "\n",
    "def training(model, df, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    train, val = data_sample(df)\n",
    "    track_val_loss = 1000\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
    "        valid_loss, valid_acc, auc = valid_metrics(model, val) \n",
    "        if i%10== 0:\n",
    "            print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc))\n",
    "        if i%2 == 0: \n",
    "            train, val = data_sample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84c30f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:51:35.864874Z",
     "start_time": "2022-02-27T04:47:06.022450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.693 train acc 0.500 valid loss 0.650 valid acc 0.805 roc auc acc 0.819\n",
      "train loss 0.235 train acc 0.938 valid loss 0.264 valid acc 0.926 roc auc acc 0.977\n",
      "train loss 0.185 train acc 0.957 valid loss 0.215 valid acc 0.942 roc auc acc 0.984\n",
      "train loss 0.178 train acc 0.964 valid loss 0.202 valid acc 0.952 roc auc acc 0.988\n",
      "train loss 0.174 train acc 0.967 valid loss 0.198 valid acc 0.955 roc auc acc 0.990\n",
      "train loss 0.172 train acc 0.968 valid loss 0.196 valid acc 0.956 roc auc acc 0.991\n",
      "train loss 0.171 train acc 0.969 valid loss 0.195 valid acc 0.957 roc auc acc 0.991\n",
      "train loss 0.171 train acc 0.969 valid loss 0.194 valid acc 0.957 roc auc acc 0.991\n",
      "train loss 0.170 train acc 0.969 valid loss 0.194 valid acc 0.957 roc auc acc 0.991\n",
      "train loss 0.171 train acc 0.969 valid loss 0.193 valid acc 0.958 roc auc acc 0.992\n",
      "train loss 0.170 train acc 0.969 valid loss 0.193 valid acc 0.958 roc auc acc 0.992\n",
      "train loss 0.170 train acc 0.969 valid loss 0.193 valid acc 0.958 roc auc acc 0.992\n",
      "train loss 0.170 train acc 0.969 valid loss 0.194 valid acc 0.958 roc auc acc 0.991\n"
     ]
    }
   ],
   "source": [
    "model_1 = MF(df.user_id.max()+1, df.item_id.max()+1, emb_size= 75)\n",
    "training(model_1, df, epochs=126, lr=.12, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89321ef1-3140-4cdd-a6bc-f121c17b4c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf319866",
   "metadata": {},
   "source": [
    "### New Model: MF3, concat context feature and item feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0402666b-788a-4f38-9c5c-8902b45bd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF3(nn.Module):\n",
    "    def __init__(self, num_users, num_items,num_context_feature ,num_item_feature, \n",
    "                 user_emb_size=20, item_emb_size=20, item_context_emb_size=8, item_feature_emb_size=8):\n",
    "        super(MF3, self).__init__()\n",
    "#         torch.manual_seed(seed)\n",
    "        self.user_emb = nn.Embedding(num_users, user_emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, item_emb_size)\n",
    "        self.context_feature_emb = nn.Embedding(num_context_feature, item_context_emb_size)\n",
    "        self.item_feature_emb = nn.Embedding(num_item_feature, item_feature_emb_size)\n",
    "        # init\n",
    "        self.item_feature_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "        self.linear1 = nn.Linear(user_emb_size+item_emb_size+item_context_emb_size+item_feature_emb_size, 5)\n",
    "        self.dropout = nn.Dropout(p = 0.05)\n",
    "        self.nonlin = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, u, v, c, i):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        C = self.context_feature_emb(c)\n",
    "        I = self.item_feature_emb(i)\n",
    "        ensemble = torch.cat((U,V,C,I),dim=1)\n",
    "        pred = self.linear1(ensemble)\n",
    "        pred = self.dropout(pred)\n",
    "        pred = self.nonlin(pred)\n",
    "        pred = self.linear2(pred)\n",
    "        return self.classifier(pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb11eeee-112c-4415-b8aa-57b9125c87b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T04:40:39.134802Z",
     "start_time": "2022-02-27T04:40:39.128366Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_df, optimizer):\n",
    "    \"\"\" Trains the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    y = torch.FloatTensor(train_df.label.values)\n",
    "    u = torch.LongTensor(train_df.user_id.values)\n",
    "    v = torch.LongTensor(train_df.item_id.values)\n",
    "    c = torch.LongTensor(train_df.context_feature_id.values)\n",
    "    i = torch.LongTensor(train_df.item_feature_id.values)\n",
    "    y_hat = model(u,v,c,i)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    train_acc = accuracy_score(output,y)\n",
    "    train_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    return train_loss.item(), train_acc\n",
    "\n",
    "def valid_metrics(model, valid_df):\n",
    "    \"\"\"Computes validation loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    u = torch.LongTensor(valid_df.user_id.values)\n",
    "    v = torch.LongTensor(valid_df.item_id.values)\n",
    "    y = torch.FloatTensor(valid_df.label.values)\n",
    "    c = torch.LongTensor(valid_df.context_feature_id.values)\n",
    "    i = torch.LongTensor(valid_df.item_feature_id.values)\n",
    "    y_hat = model(u,v,c,i)\n",
    "    valid_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    auc = roc_auc_score(y.detach().numpy(), y_hat.detach().numpy())\n",
    "    valid_acc = accuracy_score(output,y)\n",
    "    return valid_loss.item(), valid_acc, auc\n",
    "\n",
    "def training(model, df, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    train, val = data_sample(df)\n",
    "    track_val_loss = 1000\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
    "        valid_loss, valid_acc, auc = valid_metrics(model, val) \n",
    "        if i%5== 0:\n",
    "            print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc))\n",
    "        \n",
    "#         if track_val_loss <= valid_loss:\n",
    "#             break\n",
    "#         track_val_loss = valid_loss\n",
    "#         if i%2 == 0: \n",
    "#             train, val = data_sample(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d8ad4ab3-62ee-4a26-8898-a0be3d4bda28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.668 train acc 0.667 valid loss 0.646 valid acc 0.667 roc auc acc 0.577\n",
      "train loss 0.386 train acc 0.668 valid loss 0.390 valid acc 0.879 roc auc acc 0.933\n",
      "train loss 0.323 train acc 0.887 valid loss 0.362 valid acc 0.871 roc auc acc 0.933\n",
      "train loss 0.293 train acc 0.887 valid loss 0.336 valid acc 0.871 roc auc acc 0.937\n",
      "train loss 0.278 train acc 0.892 valid loss 0.312 valid acc 0.880 roc auc acc 0.940\n",
      "train loss 0.264 train acc 0.897 valid loss 0.294 valid acc 0.885 roc auc acc 0.943\n",
      "train loss 0.253 train acc 0.901 valid loss 0.291 valid acc 0.886 roc auc acc 0.944\n",
      "train loss 0.249 train acc 0.902 valid loss 0.293 valid acc 0.885 roc auc acc 0.944\n",
      "train loss 0.248 train acc 0.903 valid loss 0.292 valid acc 0.885 roc auc acc 0.945\n",
      "train loss 0.246 train acc 0.903 valid loss 0.287 valid acc 0.887 roc auc acc 0.946\n",
      "train loss 0.244 train acc 0.904 valid loss 0.280 valid acc 0.889 roc auc acc 0.948\n",
      "train loss 0.241 train acc 0.904 valid loss 0.277 valid acc 0.889 roc auc acc 0.949\n",
      "train loss 0.239 train acc 0.905 valid loss 0.277 valid acc 0.890 roc auc acc 0.949\n"
     ]
    }
   ],
   "source": [
    "model_2 = MF(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, emb_size=50)\n",
    "training(model_2, df, epochs=126, lr=.05, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2ed9eb70-1fc8-45b0-83b4-21ee0f3887fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.693 train acc 0.500 valid loss 0.693 valid acc 0.528 roc auc acc 0.559\n",
      "train loss 0.688 train acc 0.613 valid loss 0.686 valid acc 0.637 roc auc acc 0.714\n",
      "train loss 0.667 train acc 0.649 valid loss 0.661 valid acc 0.646 roc auc acc 0.744\n",
      "train loss 0.624 train acc 0.672 valid loss 0.613 valid acc 0.677 roc auc acc 0.774\n",
      "train loss 0.566 train acc 0.738 valid loss 0.555 valid acc 0.749 roc auc acc 0.827\n",
      "train loss 0.506 train acc 0.798 valid loss 0.494 valid acc 0.807 roc auc acc 0.870\n",
      "train loss 0.450 train acc 0.826 valid loss 0.442 valid acc 0.831 roc auc acc 0.897\n",
      "train loss 0.412 train acc 0.842 valid loss 0.408 valid acc 0.844 roc auc acc 0.910\n",
      "train loss 0.385 train acc 0.850 valid loss 0.385 valid acc 0.851 roc auc acc 0.916\n",
      "train loss 0.370 train acc 0.854 valid loss 0.372 valid acc 0.854 roc auc acc 0.919\n",
      "train loss 0.361 train acc 0.857 valid loss 0.366 valid acc 0.856 roc auc acc 0.920\n",
      "train loss 0.357 train acc 0.858 valid loss 0.362 valid acc 0.856 roc auc acc 0.920\n",
      "train loss 0.354 train acc 0.858 valid loss 0.360 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.353 train acc 0.859 valid loss 0.359 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.353 train acc 0.859 valid loss 0.359 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.353 train acc 0.859 valid loss 0.359 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.352 train acc 0.859 valid loss 0.359 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.352 train acc 0.859 valid loss 0.358 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.351 train acc 0.860 valid loss 0.357 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.349 train acc 0.860 valid loss 0.356 valid acc 0.858 roc auc acc 0.921\n"
     ]
    }
   ],
   "source": [
    "model_5 = MF3(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, emb_size= 75)\n",
    "training(model_5, df, epochs=100, lr=.01, wd=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aec864bf-07bc-48a5-bb49-fbad813e9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.712 train acc 0.500 valid loss 0.701 valid acc 0.501 roc auc acc 0.554\n",
      "train loss 0.689 train acc 0.576 valid loss 0.684 valid acc 0.581 roc auc acc 0.622\n",
      "train loss 0.660 train acc 0.661 valid loss 0.652 valid acc 0.678 roc auc acc 0.737\n",
      "train loss 0.618 train acc 0.694 valid loss 0.608 valid acc 0.700 roc auc acc 0.772\n",
      "train loss 0.560 train acc 0.743 valid loss 0.545 valid acc 0.750 roc auc acc 0.812\n",
      "train loss 0.488 train acc 0.809 valid loss 0.476 valid acc 0.816 roc auc acc 0.870\n",
      "train loss 0.438 train acc 0.823 valid loss 0.430 valid acc 0.831 roc auc acc 0.905\n",
      "train loss 0.403 train acc 0.840 valid loss 0.400 valid acc 0.846 roc auc acc 0.913\n",
      "train loss 0.384 train acc 0.847 valid loss 0.382 valid acc 0.851 roc auc acc 0.916\n",
      "train loss 0.371 train acc 0.851 valid loss 0.372 valid acc 0.854 roc auc acc 0.919\n",
      "train loss 0.363 train acc 0.854 valid loss 0.365 valid acc 0.856 roc auc acc 0.920\n",
      "train loss 0.357 train acc 0.855 valid loss 0.360 valid acc 0.857 roc auc acc 0.920\n",
      "train loss 0.354 train acc 0.856 valid loss 0.359 valid acc 0.857 roc auc acc 0.921\n",
      "train loss 0.353 train acc 0.857 valid loss 0.358 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.351 train acc 0.858 valid loss 0.357 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.350 train acc 0.859 valid loss 0.356 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.349 train acc 0.859 valid loss 0.355 valid acc 0.858 roc auc acc 0.921\n",
      "train loss 0.348 train acc 0.859 valid loss 0.355 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.347 train acc 0.860 valid loss 0.354 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.346 train acc 0.860 valid loss 0.353 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.345 train acc 0.860 valid loss 0.353 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.344 train acc 0.861 valid loss 0.352 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.344 train acc 0.861 valid loss 0.351 valid acc 0.859 roc auc acc 0.921\n",
      "train loss 0.343 train acc 0.861 valid loss 0.351 valid acc 0.859 roc auc acc 0.921\n"
     ]
    }
   ],
   "source": [
    "model_6 = MF3(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, \n",
    "              user_emb_size= 100, item_emb_size=130,\n",
    "              item_context_emb_size=20,item_feature_emb_size=30)\n",
    "training(model_6, df, epochs=120, lr=.01, wd=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f32fb939-5d10-40e7-9339-34f1975d5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.693 train acc 0.500 valid loss 0.690 valid acc 0.581 roc auc acc 0.664\n",
      "train loss 0.651 train acc 0.797 valid loss 0.635 valid acc 0.819 roc auc acc 0.886\n",
      "train loss 0.545 train acc 0.846 valid loss 0.521 valid acc 0.847 roc auc acc 0.902\n",
      "train loss 0.460 train acc 0.863 valid loss 0.451 valid acc 0.859 roc auc acc 0.902\n",
      "train loss 0.415 train acc 0.864 valid loss 0.409 valid acc 0.859 roc auc acc 0.909\n",
      "train loss 0.357 train acc 0.864 valid loss 0.363 valid acc 0.858 roc auc acc 0.917\n",
      "train loss 0.325 train acc 0.870 valid loss 0.349 valid acc 0.859 roc auc acc 0.922\n",
      "train loss 0.304 train acc 0.876 valid loss 0.348 valid acc 0.859 roc auc acc 0.923\n",
      "train loss 0.286 train acc 0.883 valid loss 0.352 valid acc 0.857 roc auc acc 0.922\n"
     ]
    }
   ],
   "source": [
    "model_8 = MF3(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, \n",
    "              user_emb_size= 100, item_emb_size=130,\n",
    "              item_context_emb_size=20,item_feature_emb_size=30)\n",
    "training(model_8, df, epochs=41, lr=.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "199ba0a7-e517-4336-9ffb-d059b5559a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 train acc 0.500 valid loss 0.693 valid acc 0.501 roc auc acc 0.559\n",
      "train loss 0.688 train acc 0.546 valid loss 0.686 valid acc 0.577 roc auc acc 0.722\n",
      "train loss 0.657 train acc 0.721 valid loss 0.645 valid acc 0.720 roc auc acc 0.798\n",
      "train loss 0.576 train acc 0.735 valid loss 0.557 valid acc 0.749 roc auc acc 0.858\n",
      "train loss 0.452 train acc 0.852 valid loss 0.436 valid acc 0.852 roc auc acc 0.904\n",
      "train loss 0.376 train acc 0.868 valid loss 0.378 valid acc 0.860 roc auc acc 0.916\n",
      "train loss 0.336 train acc 0.871 valid loss 0.358 valid acc 0.857 roc auc acc 0.920\n",
      "train loss 0.313 train acc 0.874 valid loss 0.356 valid acc 0.856 roc auc acc 0.921\n",
      "train loss 0.300 train acc 0.874 valid loss 0.358 valid acc 0.855 roc auc acc 0.920\n"
     ]
    }
   ],
   "source": [
    "model_9 = MF3(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, \n",
    "              user_emb_size= 40, item_emb_size=40,\n",
    "              item_context_emb_size=20,item_feature_emb_size=30)\n",
    "training(model_9, df, epochs=41, lr=.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ff1ab94-6a3e-4ef2-85e2-405c4d69d9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6507, 0.3761, 0.8977,  ..., 0.9230, 0.9230, 0.1248],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_kaggle.csv')\n",
    "test = test.merge(item, on = 'item_id', how = 'left')\n",
    "u = torch.LongTensor(test.user_id.values)\n",
    "v = torch.LongTensor(test.item_id.values)\n",
    "c = torch.LongTensor(test.context_feature_id.values)\n",
    "i = torch.LongTensor(test.item_feature_id.values)\n",
    "y_hat = model_5(u,v,c,i)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5c13c91d-4e8a-4c94-826d-ba277962a9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5306160441548566"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = pd.Series(y_hat.detach().numpy()).reset_index().rename(columns = {'index':'id',0:'rating'})\n",
    "sum(prob.rating>0.5)/len(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1a20d-0641-4a05-9864-6f8d69c5e649",
   "metadata": {},
   "source": [
    "### overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "127fa0b2-1343-4c31-a2e3-c3a831bddf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.726 train acc 0.500 valid loss 0.724 valid acc 0.500 roc auc acc 0.473\n",
      "train loss 0.716 train acc 0.500 valid loss 0.715 valid acc 0.500 roc auc acc 0.581\n",
      "train loss 0.707 train acc 0.500 valid loss 0.705 valid acc 0.500 roc auc acc 0.597\n",
      "train loss 0.691 train acc 0.500 valid loss 0.686 valid acc 0.500 roc auc acc 0.710\n",
      "train loss 0.655 train acc 0.696 valid loss 0.642 valid acc 0.701 roc auc acc 0.858\n",
      "train loss 0.577 train acc 0.733 valid loss 0.553 valid acc 0.746 roc auc acc 0.892\n",
      "train loss 0.496 train acc 0.822 valid loss 0.478 valid acc 0.847 roc auc acc 0.900\n",
      "train loss 0.450 train acc 0.857 valid loss 0.438 valid acc 0.859 roc auc acc 0.902\n",
      "train loss 0.416 train acc 0.862 valid loss 0.412 valid acc 0.858 roc auc acc 0.899\n",
      "train loss 0.360 train acc 0.869 valid loss 0.368 valid acc 0.857 roc auc acc 0.911\n",
      "train loss 0.315 train acc 0.880 valid loss 0.363 valid acc 0.853 roc auc acc 0.918\n",
      "train loss 0.296 train acc 0.888 valid loss 0.379 valid acc 0.848 roc auc acc 0.914\n",
      "train loss 0.289 train acc 0.891 valid loss 0.389 valid acc 0.844 roc auc acc 0.914\n",
      "train loss 0.281 train acc 0.895 valid loss 0.395 valid acc 0.840 roc auc acc 0.913\n",
      "train loss 0.274 train acc 0.898 valid loss 0.395 valid acc 0.839 roc auc acc 0.914\n",
      "train loss 0.268 train acc 0.902 valid loss 0.400 valid acc 0.836 roc auc acc 0.915\n",
      "train loss 0.263 train acc 0.903 valid loss 0.404 valid acc 0.834 roc auc acc 0.916\n"
     ]
    }
   ],
   "source": [
    "model_7 = MF3(df.user_id.max()+1, df.item_id.max()+1, \n",
    "              df.context_feature_id.max()+1, df.item_feature_id.max()+1, user_emb_size=100, item_emb_size=120,\n",
    "              item_context_emb_size=16,item_feature_emb_size=20)\n",
    "training(model_7, df, epochs=81, lr=.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80fca5",
   "metadata": {},
   "source": [
    "### Extract model parmeters and replace with average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4766174f-5c49-45ec-8caa-2932a7204d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6507],\n",
       "        [0.3761],\n",
       "        [0.8977],\n",
       "        ...,\n",
       "        [0.9230],\n",
       "        [0.9230],\n",
       "        [0.1248]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_kaggle.csv')\n",
    "test = test.merge(item, on = 'item_id', how = 'left')\n",
    "para = [i for i in model_5.parameters()]\n",
    "user_mean = torch.mean(para[0], dim = 0)\n",
    "item_mean = torch.mean(para[1], dim = 0)\n",
    "user_context_mean = torch.mean(para[2], dim=0)\n",
    "item_feature_mean = torch.mean(para[3], dim=0)\n",
    "\n",
    "\n",
    "x = []\n",
    "train_user = set(train.user_id)\n",
    "for i in test.user_id:\n",
    "    if i in train_user:\n",
    "        x.append(para[0][i])\n",
    "    else:\n",
    "        x.append(user_mean)\n",
    "x = torch.stack(x)\n",
    "\n",
    "y = []\n",
    "train_item = set(train.item_id)\n",
    "for i in test.item_id:\n",
    "    if i in train_item:\n",
    "        y.append(para[1][i])\n",
    "    else:\n",
    "        y.append(item_mean)\n",
    "y = torch.stack(y)\n",
    "\n",
    "w = []\n",
    "train_user_context = set(train.context_feature_id)\n",
    "for i in test.context_feature_id:\n",
    "    if i in train_user_context:\n",
    "        w.append(para[2][i])\n",
    "    else:\n",
    "        w.append(user_context_mean)\n",
    "        \n",
    "w = torch.stack(w)\n",
    "\n",
    "z = []\n",
    "train_item_feature = set(train.item_feature_id)\n",
    "for i in test.item_feature_id:\n",
    "    if i in train_item_feature:\n",
    "        z.append(para[3][i])\n",
    "    else:\n",
    "        z.append(item_feature_mean)\n",
    "        \n",
    "z = torch.stack(z)\n",
    "\n",
    "ensemble = torch.cat((x,y,w,z),dim=1)\n",
    "pred = torch.matmul(ensemble, para[4].T)+para[5]\n",
    "pred = torch.nn.functional.leaky_relu(pred)\n",
    "pred = torch.matmul(pred, para[6].T)+para[7]\n",
    "pred = torch.sigmoid(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6f56f380-79e4-4e11-b365-54ebf638d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = pd.Series(pred.squeeze().detach().numpy()).reset_index().rename(columns = {'index':'id',0:'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54305bcf-d1ba-4448-b8e0-a5ba5ca20b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.to_csv('trial54.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
