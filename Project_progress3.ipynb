{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909ab91f-23b4-4d8f-a807-e5129cf421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ea2ac-d9e4-4c81-ae8c-1d51200d0510",
   "metadata": {},
   "source": [
    "### MF Model: More Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4645e803-c84c-469f-8e54-f42b5c5ca5a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1644645329784,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "5f7GoYC8K1M2"
   },
   "outputs": [],
   "source": [
    "item = pd.read_csv('item_feature.csv')\n",
    "train = pd.read_csv('training.csv')\n",
    "df = train.merge(item, on = 'item_id', how = 'left')\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa635bac-5d65-44d0-84ab-4fa546866833",
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1644645333677,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "hg1eeAaMLNu1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random Negative Sample\n",
    "u = np.random.randint(low=0.0, high=df.user_id.max(), size=int(len(df)))\n",
    "i = np.random.randint(low=0.0, high=df.item_id.max(), size=int(len(df)))\n",
    "c = np.random.randint(low=0.0, high=df.context_feature_id.max(), size=int(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4faf0e0-8a09-469d-8008-812306d9a0bd",
   "metadata": {
    "executionInfo": {
     "elapsed": 5291,
     "status": "ok",
     "timestamp": 1644645339795,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "_LrBIlsILT_b"
   },
   "outputs": [],
   "source": [
    "sample= pd.concat([pd.Series(u),pd.Series(i),pd.Series(c)], axis =1).\\\n",
    "rename(columns={0:'user_id', 1:'item_id', 2:'context_feature_id'})\n",
    "sample = sample.merge(item, on = 'item_id', how = 'left')\n",
    "sample['label'] = 0\n",
    "df = pd.concat([df,sample])\n",
    "df = df.drop_duplicates(subset=['user_id','item_id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0736bbd-944d-40cc-9930-e3af2483041d",
   "metadata": {
    "id": "G5aUDosLLXAM"
   },
   "outputs": [],
   "source": [
    "pos = df[df.label ==1].reset_index(drop = True)\n",
    "neg = df[df.label ==0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80f4aea1-e8de-49e0-83d5-15364d24f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sample(data):\n",
    "\n",
    "    train=data.sample(frac=0.8)\n",
    "    val=data.drop(train.index)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb55ae5-d428-4d3c-ad36-e2a4b416c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF2(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=20):\n",
    "        super(MF2, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "        self.nonlin = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p = 0.1)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        U = self.nonlin(U)\n",
    "        V = self.item_emb(v)\n",
    "        V = self.drop(V)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        return self.classifier((U*V).sum(1) +  b_u  + b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75fa200c-9eac-491e-a642-b1449619ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_df, optimizer):\n",
    "    \"\"\" Trains the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    y = torch.FloatTensor(train_df.label.values)\n",
    "    u = torch.LongTensor(train_df.user_id.values)\n",
    "    v = torch.LongTensor(train_df.item_id.values)\n",
    "    y_hat = model(u,v)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    train_acc = accuracy_score(output,y)\n",
    "    train_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    return train_loss.item(), train_acc\n",
    "\n",
    "def valid_metrics(model, valid_df):\n",
    "    \"\"\"Computes validation loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    u = torch.LongTensor(valid_df.user_id.values)\n",
    "    v = torch.LongTensor(valid_df.item_id.values)\n",
    "    y = torch.FloatTensor(valid_df.label.values)\n",
    "    y_hat = model(u,v)\n",
    "    valid_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    auc = roc_auc_score(y.detach().numpy(), y_hat.detach().numpy())\n",
    "    valid_acc = accuracy_score(output,y)\n",
    "    return valid_loss.item(), valid_acc, auc\n",
    "\n",
    "def training(model, df, epochs=10, lr=0.01, wd=0.0, whole=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    if whole:\n",
    "        train = df\n",
    "    else:\n",
    "        train, val = data_sample(df)\n",
    "    for i in range(epochs):\n",
    "        if whole:\n",
    "            train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
    "            if i%10== 0:\n",
    "                print(\"train loss %.3f train acc %.3f\" % (train_loss,train_acc))\n",
    "        else:\n",
    "            train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
    "            valid_loss, valid_acc, auc = valid_metrics(model, val) \n",
    "            if i%10== 0:\n",
    "                print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc)) \n",
    "#         if i%2 == 0: \n",
    "#             train, val = data_sample(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cfa3cb5-3d88-45dd-9816-3303aee9fa88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 train acc 0.471 valid loss 0.652 valid acc 0.823 roc auc acc 0.869\n",
      "train loss 0.222 train acc 0.926 valid loss 0.357 valid acc 0.870 roc auc acc 0.927\n",
      "train loss 0.190 train acc 0.966 valid loss 0.333 valid acc 0.879 roc auc acc 0.941\n",
      "train loss 0.174 train acc 0.968 valid loss 0.315 valid acc 0.884 roc auc acc 0.945\n",
      "train loss 0.169 train acc 0.972 valid loss 0.306 valid acc 0.889 roc auc acc 0.948\n",
      "train loss 0.165 train acc 0.974 valid loss 0.298 valid acc 0.893 roc auc acc 0.951\n",
      "train loss 0.163 train acc 0.975 valid loss 0.292 valid acc 0.897 roc auc acc 0.953\n",
      "train loss 0.162 train acc 0.976 valid loss 0.288 valid acc 0.900 roc auc acc 0.955\n",
      "train loss 0.161 train acc 0.976 valid loss 0.285 valid acc 0.902 roc auc acc 0.956\n",
      "train loss 0.161 train acc 0.977 valid loss 0.283 valid acc 0.904 roc auc acc 0.957\n",
      "train loss 0.161 train acc 0.977 valid loss 0.281 valid acc 0.905 roc auc acc 0.958\n",
      "train loss 0.160 train acc 0.977 valid loss 0.281 valid acc 0.906 roc auc acc 0.958\n",
      "train loss 0.160 train acc 0.977 valid loss 0.280 valid acc 0.906 roc auc acc 0.958\n"
     ]
    }
   ],
   "source": [
    "model = MF2(df.user_id.max()+1, df.item_id.max()+1, emb_size=85) # our best result with test data\n",
    "training(model, df, epochs=126, lr=.06, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0604c724-e7a9-4f53-a121-c812e1d96e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695 train acc 0.471\n",
      "train loss 0.239 train acc 0.915\n",
      "train loss 0.205 train acc 0.958\n",
      "train loss 0.186 train acc 0.962\n",
      "train loss 0.181 train acc 0.966\n",
      "train loss 0.176 train acc 0.969\n",
      "train loss 0.174 train acc 0.970\n",
      "train loss 0.173 train acc 0.971\n",
      "train loss 0.172 train acc 0.971\n",
      "train loss 0.172 train acc 0.971\n",
      "train loss 0.172 train acc 0.972\n",
      "train loss 0.172 train acc 0.972\n",
      "train loss 0.172 train acc 0.972\n"
     ]
    }
   ],
   "source": [
    "model = MF2(df.user_id.max()+1, df.item_id.max()+1, emb_size=85) # our best result with test data\n",
    "training(model, df, epochs=126, lr=.06, wd=1e-6, whole = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "648880ca-6e88-4aae-8cfd-f6baa46e3242",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1644641673342,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "LESGoHr2RMe7",
    "outputId": "ea320880-220b-44d3-d194-c565babeaecc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3890, 0.3308, 0.7549,  ..., 0.8805, 0.8728, 0.1300],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_kaggle.csv')\n",
    "u = torch.LongTensor(test.user_id.values)\n",
    "v = torch.LongTensor(test.item_id.values)\n",
    "y_hat = model(u,v)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbf3d4f3-5049-40ea-aa9c-d94e82c8e745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1644641677417,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "37Jt5uhtR_sx",
    "outputId": "c137b8ec-958b-4f0f-ac54-77be9a30af93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45943338096673964"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = pd.Series(y_hat.detach().numpy()).reset_index().rename(columns = {'index':'id',0:'rating'})\n",
    "sum(prob.rating>0.5)/len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bf05ed6-3eb8-4849-b33d-8fb76be369f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2566,
     "status": "ok",
     "timestamp": 1644641714431,
     "user": {
      "displayName": "Zhao Kaihang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12474067113521262852"
     },
     "user_tz": 480
    },
    "id": "BtFR1a08SCl0"
   },
   "outputs": [],
   "source": [
    "prob.to_csv('trial77.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59807a-98e6-4cdb-80c1-deef7318fd07",
   "metadata": {},
   "source": [
    "### NN Model: The best result of 0.411\n",
    "\n",
    "Added a break condition when val_loss<0.3 to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc16ef-0005-4266-af17-390738d85333",
   "metadata": {
    "id": "5q-J-95bKv8p"
   },
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598173b-732a-4427-afd0-fb0a9b3e8f57",
   "metadata": {
    "id": "5f7GoYC8K1M2"
   },
   "outputs": [],
   "source": [
    "item = pd.read_csv('item_feature.csv')\n",
    "train = pd.read_csv('training.csv')\n",
    "df = train.merge(item, on = 'item_id', how = 'left')\n",
    "df['label'] =1\n",
    "train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8160b-be89-40e5-837d-19a5a70df3ae",
   "metadata": {
    "id": "hg1eeAaMLNu1"
   },
   "outputs": [],
   "source": [
    "u = np.random.randint(low=0.0, high=df.user_id.max(), size=int(len(df)*2))\n",
    "i = np.random.randint(low=0.0, high=df.item_id.max(), size=int(len(df)*2))\n",
    "p = (df.context_feature_id.value_counts()/len(df)).sort_index().values\n",
    "p[p.argmin()]+=1-p.sum()\n",
    "c = np.random.choice(4, len(df)*10, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfe201-b73b-4732-9a84-75a9fcb02254",
   "metadata": {
    "id": "_LrBIlsILT_b"
   },
   "outputs": [],
   "source": [
    "sample= pd.concat([pd.Series(u),pd.Series(i)], axis =1).\\\n",
    "rename(columns={0:'user_id', 1:'item_id'})\n",
    "sample = sample.merge(item, on = 'item_id', how = 'left')\n",
    "sample['label'] = 0\n",
    "df = pd.concat([df,sample])\n",
    "df = df.drop_duplicates(subset=['user_id','item_id']).reset_index(drop = True)\n",
    "df = df[df['label']==0]\n",
    "df = pd.concat([train, df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027fa70-de3e-4045-9607-5eea2965e4c7",
   "metadata": {
    "id": "G5aUDosLLXAM"
   },
   "outputs": [],
   "source": [
    "pos = df[df.label ==1].reset_index(drop = True)\n",
    "neg = df[df.label ==0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8d725-4a01-465d-baaf-3caf36628bb3",
   "metadata": {
    "id": "-Q4n_DLmNxin"
   },
   "outputs": [],
   "source": [
    "def data_sample(pos, neg):\n",
    "\n",
    "  msk = np.random.rand(len(pos)) < 0.8\n",
    "  train_pos = pos[msk].reset_index(drop = True)\n",
    "  val_pos = pos[~msk].reset_index(drop = True)\n",
    "\n",
    "  msk = np.random.rand(len(neg)) < 0.8\n",
    "  train_neg = neg[msk].sample(frac = len(pos)/len(neg)).reset_index(drop = True)\n",
    "  val_neg = neg[~msk].sample(frac = len(pos)/len(neg)).reset_index(drop = True)\n",
    "\n",
    "  train = pd.concat([train_pos, train_neg]).sample(frac=1).reset_index(drop = True)\n",
    "  val = pd.concat([val_pos, val_neg]).sample(frac=1).reset_index(drop = True)\n",
    "  \n",
    "  return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e48766-e71f-4afd-aa50-9ea48572012b",
   "metadata": {
    "id": "tLmH4KX1MLzE"
   },
   "outputs": [],
   "source": [
    "train, val = data_sample(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2174b4f-b954-4dee-a92f-02576a5a3f51",
   "metadata": {
    "id": "5E0mlmLbPZ5u"
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_item_feature, emb_user_size=20, \n",
    "                 emb_item_size=20, emb_item_feature_size=20):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_user_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_item_size)\n",
    "        self.item_feature_emb = nn.Embedding(num_item_feature, emb_item_feature_size)\n",
    "        # init \n",
    "        self.item_feature_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "        self.linear1 = nn.Linear(emb_user_size+emb_item_size+emb_item_feature_size, 2)\n",
    "        self.nonlin = nn.LeakyReLU()\n",
    "        self.linear3 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, u, v, c):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        C = self.item_feature_emb(c)\n",
    "        ensemble = torch.cat((U,V,C),dim=1)\n",
    "        pred = self.linear1(ensemble)\n",
    "        pred = self.nonlin(pred)\n",
    "        pred = self.linear3(pred)\n",
    "        return self.classifier(pred.squeeze())\n",
    "\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        return self.U[idx], self.V[idx], self.C[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42fd5f-f043-426a-bc35-aff2cc784b61",
   "metadata": {
    "id": "HKAZM17zPi5o"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_df, optimizer):\n",
    "  tensor_x_tr = torch.LongTensor(np.array(train_df[['user_id', 'item_id', 'item_feature_id']])) # transform to torch tensor\n",
    "  tensor_y_tr = torch.Tensor(train_df['label'])\n",
    "  train_ds = TensorDataset(tensor_x_tr,tensor_y_tr)\n",
    "  train_dl = DataLoader(train_ds, batch_size=50000, shuffle=True)\n",
    "  \"\"\" Trains the model for one epoch\"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  losses = []\n",
    "  acc=[]\n",
    "  for x, y in train_dl:\n",
    "      model.train()\n",
    "      y = y.cuda()\n",
    "      u = torch.LongTensor(x[:,0]).cuda()\n",
    "      v = torch.LongTensor(x[:,1]).cuda()\n",
    "      c = torch.LongTensor(x[:,2]).cuda()\n",
    "      y_hat = model(u,v,c)\n",
    "      output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "      loss = F.binary_cross_entropy(y_hat, y.float())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss.item())\n",
    "      acc.append(accuracy_score(output.cpu().detach().numpy(),y.cpu().detach().numpy()))\n",
    "      \n",
    "  train_loss = np.mean(losses)\n",
    "  train_acc = np.mean(acc)\n",
    "\n",
    "    ### END SOLUTION\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def valid_metrics(model, valid_df):\n",
    "    \"\"\"Computes validation loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    ### BEGIN SOLUTION\n",
    "    u = torch.LongTensor(valid_df.user_id.values).cuda()\n",
    "    v = torch.LongTensor(valid_df.item_id.values).cuda()\n",
    "    c = torch.LongTensor(valid_df.item_feature_id.values).cuda()\n",
    "    y = torch.FloatTensor(valid_df.label.values).cuda()\n",
    "    y_hat = model(u,v,c)\n",
    "    valid_loss = F.binary_cross_entropy(y_hat, y)\n",
    "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
    "    auc = roc_auc_score(y.cpu().detach().numpy(), y_hat.cpu().detach().numpy())\n",
    "    valid_acc = accuracy_score(output.cpu().detach().numpy(),y.cpu().detach().numpy())\n",
    "    ### END SOLUTION\n",
    "    return valid_loss.item(), valid_acc, auc\n",
    "\n",
    "def training(model, pos, neg, epochs=10, lr=0.01, wd=0.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    train, val = data_sample(pos, neg)\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
    "        valid_loss, valid_acc, auc = valid_metrics(model, val) \n",
    "        if valid_loss<0.3: \n",
    "          print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc)) \n",
    "          break\n",
    "        if i%2 == 0: \n",
    "          train, val = data_sample(pos, neg)\n",
    "        if i%5 == 0:\n",
    "          print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc)) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613e209-6e3a-4ce9-a61b-35b1e8e1c481",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g17oolHePk_t",
    "outputId": "9cde5c6a-cae3-41b9-b9c9-c66e7a5abd50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.749 train acc 0.500 valid loss 0.746 valid acc 0.500 roc auc acc 0.886\n",
      "train loss 0.647 train acc 0.500 valid loss 0.627 valid acc 0.500 roc auc acc 0.935\n",
      "train loss 0.429 train acc 0.875 valid loss 0.414 valid acc 0.874 roc auc acc 0.945\n",
      "train loss 0.320 train acc 0.879 valid loss 0.314 valid acc 0.880 roc auc acc 0.954\n",
      "train loss 0.305 train acc 0.883 valid loss 0.307 valid acc 0.881 roc auc acc 0.954\n"
     ]
    }
   ],
   "source": [
    "model = MF(df.user_id.max()+1, df.item_id.max()+1, df.item_feature_id.max()+1,\n",
    "           emb_user_size=32, emb_item_size=24, emb_item_feature_size=8).cuda()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "training(model, pos, neg, epochs=51, lr=0.0005, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c038d9-0bc0-4503-bb8e-b5e0f0f4fbe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzQvFwfSzenJ",
    "outputId": "74908cb8-3ac7-4c3e-b426-2e8170048061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.300 train acc 0.884 valid loss 0.300 valid acc 0.883 roc auc acc 0.956\n"
     ]
    }
   ],
   "source": [
    "# stop, but continue training until a second stop\n",
    "training(model, pos, neg, epochs=51, lr=0.0001, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd11db-f525-4938-a4bb-dc29d184a741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCqYuDgCuCQ3",
    "outputId": "2207a5a9-73b7-4323-d2eb-e64121dfa574"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5350, 0.2149, 0.8678,  ..., 0.9268, 0.9268, 0.1153], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_kaggle.csv')\n",
    "test = test.merge(item, on = 'item_id', how = 'left')\n",
    "u = torch.LongTensor(test.user_id.values).cuda()\n",
    "v = torch.LongTensor(test.item_id.values).cuda()\n",
    "c = torch.LongTensor(test.item_feature_id.values).cuda()\n",
    "y_hat = model(u,v,c)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53670b05-8ed4-4f88-8a6a-e43a40789e80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbmsnpTq-_8i",
    "outputId": "f6467ac2-36ca-45f5-8f7f-f8cabd79c87f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51798266"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_hat.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc0865-849c-4ce8-84c0-71aed7b183a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JX-DhDffH2QR",
    "outputId": "8c1869e5-2a9e-4372-9e5f-9602e76de954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012703698362547"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = pd.Series(y_hat.cpu().detach().numpy()).reset_index().rename(columns = {'index':'id',0:'rating'})\n",
    "sum(prob.rating>0.5)/len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202906a-ed9d-4102-aa45-89b713ad5e2e",
   "metadata": {
    "id": "EjOusz6QTBzC"
   },
   "outputs": [],
   "source": [
    "prob.to_csv('trial63.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b11759-06cb-4237-83cc-45e0a3203489",
   "metadata": {},
   "source": [
    "### Ensemble our best results\n",
    "\n",
    "Decreased the variance by taking average of our trials whose scores are under around 0.415."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384812b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:03:56.039671Z",
     "start_time": "2022-03-04T04:03:55.632813Z"
    }
   },
   "outputs": [],
   "source": [
    "t77 = pd.read_csv('trial77.csv')\n",
    "t40 = pd.read_csv('trial40.csv')\n",
    "t41 = pd.read_csv('trial41.csv')\n",
    "t63 = pd.read_csv('trial63.csv')\n",
    "t68 = pd.read_csv('trial68.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fafb320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:04:53.650615Z",
     "start_time": "2022-03-04T04:04:53.632278Z"
    }
   },
   "outputs": [],
   "source": [
    "t78 = t77.copy()\n",
    "t78.rating = (t77.rating+t40.rating+t41.rating+t63.rating+t68.rating)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc07535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:05:13.565543Z",
     "start_time": "2022-03-04T04:05:13.525752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    381385.000000\n",
       "mean          0.002731\n",
       "std           0.025649\n",
       "min          -0.231606\n",
       "25%          -0.010564\n",
       "50%           0.003496\n",
       "75%           0.016680\n",
       "max           0.245898\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t78.rating - t77.rating).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec88077e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:05:52.108417Z",
     "start_time": "2022-03-04T04:05:51.435664Z"
    }
   },
   "outputs": [],
   "source": [
    "t78.to_csv('trial78.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5a7a6-4afe-4516-a615-460b804a88ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
